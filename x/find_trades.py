import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

def scrape_trades(trade_type='stock', max_pages=0):
    """
    Args:
        trade_type (str): 'stock' (è‚¡ç¥¨) æˆ– 'option' (æœŸæƒ)
        max_pages (int): 0 è¡¨ç¤ºæŠ“å–æ‰€æœ‰é¡µï¼›å¤§äº0 è¡¨ç¤ºæŠ“å–æŒ‡å®šé¡µæ•°
    """

    # --- 1. é…ç½®åŒºåŸŸï¼šIDæ˜ å°„ ---
    # å¦‚æœç½‘é¡µæ›´æ–°å¯¼è‡´ ID å˜äº†ï¼Œåªéœ€è¦ä¿®æ”¹è¿™é‡Œ
    config = {
        'stock': {
            'table_id': 'footable_8078',  # è‚¡ç¥¨è¡¨æ ¼ ID
            'filename': 'stock_trades_all.csv'
        },
        'option': {
            'table_id': 'footable_8185',  # æœŸæƒè¡¨æ ¼ ID (æå–è‡ªä½ æä¾›çš„XPath)
            'filename': 'option_trades_all.csv'
        }
    }

    if trade_type not in config:
        print(f"é”™è¯¯ï¼šä¸æ”¯æŒçš„ç±»å‹ '{trade_type}'ã€‚è¯·ä½¿ç”¨ 'stock' æˆ– 'option'ã€‚")
        return

    current_config = config[trade_type]
    target_table_id = current_config['table_id']
    target_filename = current_config['filename']

    # --- 2. æµè§ˆå™¨è®¾ç½® ---
    options = webdriver.ChromeOptions()
    # options.add_argument('--headless') # æƒ³è¦åå°é™é»˜è¿è¡Œå°±å–æ¶ˆæ³¨é‡Š
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    options.add_argument(
        'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')

    print(f"ğŸš€ å¯åŠ¨ä»»åŠ¡ï¼šæŠ“å– [{trade_type}] æ•°æ®")
    print(f"ğŸ¯ ç›®æ ‡è¡¨æ ¼ ID: {target_table_id}")
    print(f"ğŸ“„ è®¡åˆ’é¡µæ•°: {'å…¨éƒ¨ (æ— é™ç¿»é¡µ)' if max_pages == 0 else max_pages}")

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

    url = "https://findingyouredge.org/trades/"
    driver.get(url)

    # ç­‰å¾…é¡µé¢åŠ è½½
    time.sleep(5)

    all_data = []
    page_num = 1

    try:
        while True:
            print(f"--- æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ ---")
            # --- 3. æŠ“å–ä¸æ¸…æ´—æ•°æ® (å·²ä¿®å¤ä¹±ç é¡µè„š) ---
            try:
                # A. è·å–è¡¨æ ¼çš„ HTML
                table_element = driver.find_element(By.ID, target_table_id)
                table_html = table_element.get_attribute('outerHTML')

                # B. ã€æ–°å¢æ­¥éª¤ã€‘ä½¿ç”¨ BeautifulSoup å‰”é™¤ tfoot (ç¿»é¡µæ¡å°±åœ¨è¿™é‡Œé¢)
                soup = BeautifulSoup(table_html, 'html.parser')
                if soup.tfoot:
                    soup.tfoot.decompose()  # è¿™ä¸€åˆ€ä¸‹å»ï¼Œåº•éƒ¨çš„ä¹±ç è¡Œå°±æ²¡äº†

                # C. è¯»å–æ¸…æ´—åçš„ HTML
                df_current = pd.read_html(str(soup), header=0)[0]

                # D. å¤„ç†å¤šçº§è¡¨å¤´ (MultiIndex)
                if isinstance(df_current.columns, pd.MultiIndex):
                    df_current.columns = df_current.columns.get_level_values(0)

                # E. æ¸…æ´—åˆ—å
                df_current.columns = [str(c).strip() for c in df_current.columns]

                # F. ã€åŒé‡ä¿é™©ã€‘è¿‡æ»¤æ‰æ²¡æœ‰æ—¥æœŸçš„è¡Œ (é˜²æ­¢è¿˜æœ‰æ¼ç½‘ä¹‹é±¼)
                # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯â€œå¼€ä»“æ—¶é—´â€ï¼Œå¿…é¡»åŒ…å«æ•°å­—æˆ– '/'
                first_col = df_current.columns[0]
                df_current = df_current[df_current[first_col].astype(str).str.len() > 3]

                all_data.append(df_current)
                print(f"âœ… ç¬¬ {page_num} é¡µæŠ“å–æˆåŠŸï¼Œæœ¬é¡µ {len(df_current)} æ¡ã€‚")

            except Exception as e:
                print(f"âŒ æ•°æ®æŠ“å–å¤±è´¥: {e}")
                break

            # --- 4. åˆ¤æ–­æ˜¯å¦åœæ­¢ ---
            # å¦‚æœ max_pages ä¸ä¸º 0ï¼Œä¸”å½“å‰é¡µç  >= ç›®æ ‡é¡µç ï¼Œåˆ™åœæ­¢
            if max_pages > 0 and page_num >= max_pages:
                print(f"å·²è¾¾åˆ°è®¾å®šé¡µæ•° ({max_pages})ï¼Œåœæ­¢æŠ“å–ã€‚")
                break

            # --- 5. ç¿»é¡µé€»è¾‘ ---
            try:
                # åŠ¨æ€å®šä½ï¼šåœ¨è¯¥è¡¨æ ¼ ID å†…éƒ¨å¯»æ‰¾å†…å®¹ä¸º 'â€º' çš„é“¾æ¥
                # è¿™ç§å†™æ³•æ¯” li[7] æ›´ç¨³å®šï¼Œæ— è®ºå®ƒåœ¨ç¬¬å‡ ä¸ªä½ç½®éƒ½èƒ½æ‰¾åˆ°
                xpath_next = f'//*[@id="{target_table_id}"]//tfoot//ul/li/a[contains(text(), "â€º")]'

                next_btn = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.XPATH, xpath_next))
                )

                # æ£€æŸ¥æŒ‰é’®æ˜¯å¦è¢«ç¦ç”¨ (é€šå¸¸çˆ¶çº§ li ä¼šæœ‰ disabled ç±»)
                parent_li = next_btn.find_element(By.XPATH, "./..")
                if "disabled" in parent_li.get_attribute("class"):
                    print("ğŸ›‘ å·²åˆ°è¾¾æœ€åä¸€é¡µ (ç¿»é¡µæŒ‰é’®ç¦ç”¨)ã€‚")
                    break

                # æ‰§è¡Œç‚¹å‡»
                driver.execute_script("arguments[0].scrollIntoView();", next_btn)
                time.sleep(1)
                driver.execute_script("arguments[0].click();", next_btn)

                # ç­‰å¾…åŠ è½½
                print("â³ ç¿»é¡µä¸­ï¼Œç­‰å¾…æ•°æ®åˆ·æ–°...")
                time.sleep(4)
                page_num += 1

            except Exception as e:
                print("ğŸ›‘ æœªæ‰¾åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œæˆ–å·²æ˜¯æœ€åä¸€é¡µã€‚")
                break

    finally:
        driver.quit()

    # --- 6. ä¿å­˜æ•°æ® ---
    if all_data:
        try:
            print("æ­£åœ¨åˆå¹¶æ•°æ®...")
            final_df = pd.concat(all_data, ignore_index=True, sort=False)

            # ä¿å­˜ä¸º CSV (utf-8-sig é˜²æ­¢ä¸­æ–‡ä¹±ç )
            final_df.to_csv(target_filename, index=False, encoding='utf-8-sig')

            print(f"ğŸ‰ å…¨éƒ¨å®Œæˆï¼å…± {len(final_df)} æ¡æ•°æ®ã€‚")
            print(f"ğŸ’¾ æ–‡ä»¶å·²ä¿å­˜ä¸º: {target_filename}")

        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶æ—¶æŠ¥é”™: {e}")
    else:
        print("âš ï¸ æœªè·å–åˆ°ä»»ä½•æ•°æ®ã€‚")


# --- è¿™é‡Œæ˜¯ç¨‹åºçš„å…¥å£ ---
if __name__ == "__main__":
    # åœºæ™¯ 1: ä¸‹è½½ã€è‚¡ç¥¨ã€‘çš„æ‰€æœ‰é¡µ
    scrape_trades(trade_type='stock', max_pages=0)

    # åœºæ™¯ 2: ä¸‹è½½ã€æœŸæƒã€‘çš„æ‰€æœ‰é¡µ (æ ¹æ®ä½ çš„è¦æ±‚ï¼Œç”¨è¿™ä¸ª)
    # scrape_trades(trade_type='option', max_pages=0)

    # åœºæ™¯ 3: åªæµ‹è¯•ä¸‹è½½ã€æœŸæƒã€‘çš„å‰ 2 é¡µ
    # scrape_trades(trade_type='option', max_pages=2)